---
title: "CSML1000 Winter 2020, Group 8, Course Project"
author: "Steven Wang, Tarun Bagga, Paul Doucet, Jerry Khidaroo, Nikola Stevanovic"
date: "3/19/2020"
# output: 
#   html_document:
#   toc: TRUE
#   toc_depth: 2
# runtime: shiny
output:
  pdf_document:
    toc: TRUE
    toc_depth: 3
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r, message = FALSE}
# Load packages
library(knitr)
library(arules)
library(arulesViz)
library(dplyr)
library(data.table)
library(shinydashboard)
library(shiny)
library(ggplot2)
library(lubridate)
#library(Hmisc)
#library(funModeling) 
library(tidyverse)
library(survival)
library(broom)

library(survival)
library(ranger)
library(ggplot2)
library(dplyr)
library(ggfortify)
library(DataExplorer)
```

## 1. Business Understanding

- Business Problem:
  The business problem we are trying to solve fir the final project is to facioptimizing travel agent bookings so that they can accurately gauge when  the surge will occur for the consummers to fly towards sun destinations and make a profit while doing so. To do so we will build a model that will  take historical travel data and use that to let travel agents know how far in advance they should prebook hotel rooms in popular travel locations.

- Project Plan: 
  - Load and get an understanding of the dataset, its target variable and its features.
  - Make any modifications to the dataset needed to enable learning algorithms to be run on the data.
  - Identify the features of the dataset that are important in predicting the target variable (dianosis in this case).
  - Build and evaluate a few models from the dataset by appling various machine learning algoritms as appropiate and    testing them.
  - Identify the best model to use for the project.
  - Build a shiny app that deploys the selected model with a user interface for end users to imput measurement values from  a study and obtain a pridiction result.
  - Identify any ethical considerations that should be addressed at each stage of the process.

- Business Success Criteria: 
  Success criteria for this business case would be that travel agents can use the application to streamline their booking process and also use the data provided to increase profitability by booking aggressively at hotels before surge takes place.

- Ethical Framework Questions: 
  - How could your system negatively impact individuals?
    This is very important question to ask as we have to look at how our recommendations are not creating biases or undesired negative results for the customers.As this system is based on historical data, some years might have had outside factors that skew the data and while an effort has been made to remove outliers there is still a chance that a booking date will be shown that is not the best predicted date for a possible destination.
  
  - Who is most vulnerable and why? 
    The most vulnerable people are the travel agents who would use the application, a common use case shall be that the booking agents try to prebook rooms expecting a lot of sales to take place in the coming months, in case the model has not predicted an extreme event to take place such as pandemic or a weather related event then they might not reach the expected amount of sales and will either lose money or not profit as much as possible.
  
  - How much error in predictions can your business accept for this use case? 
  As this could potentially affect the business profit and bottom line, we need to try and keep error to a minimum.
  
  - Will you need to explain which input factors had the greatest influence on outputs? 
  Yes, the important input factors will need to be explained as the model is time to event and we need to imagine where the data is coming from and data.
  
  - Do you need PII or can you provide group-level data? The analysis requires customers travel booking and booking costs data however any PII can be anonymised.

## 2. Data Understanding

- Ethical Framework Questions: 
  - Have you de-identified your data and taken measures to reduce the probability of reidentification? 
    Yes the data is de-identified
      
  - Will socially sensitive features like gender or ethnic background influence outputs? 
  No, socially sensitive features will not influence outputs
    
  - Are seemingly harmless features like location hiding proxies for socially sensitive features? 
  No, there are no socially sensitive features

#### 2.1 Get Data File

- For this final project we wanted to look at a real use case which presented real issues with data with skewness, data imbalance and ambiguity we came across the vaccations Dataset used is obtained from: Air Canada Hackathon Data

#### 2.2 Load and check data

```{r, message = FALSE}
# Load data
unzip("./input/acme-travel-vacation.zip", exdir = "input") # unzip file
raw = read.table("./input/acme-travel-vacation.csv", sep="\t", header=TRUE)

# Load in airport dataset
raw_airport = read.csv("./input/cities_IATA_long_lat.csv", header=TRUE)
dataset_airport <- raw_airport

# check data
str(raw)
```
#### 2.3 Initial Data Collection Report:

  - There is two files provided as part of the given dataset for this semi supervised learning model:
    1. acme-travel-vacation.csv
    2. cities_IATA_long_lat.csv
      Each of the datset contains different type of times series data for historical bookeing oif sun destination hotels by various travel operators.
      
## Let us explore and deep dive within the various datasets provided.
    
    For this section there are 2 CSV files provided, namely acme-travel-vacation and cities_IATA_long_lat. The data inside the files specifies which is the booking and travel informatiion over the period of 5 years.
Let us look the counts of records inside the files. Using the built-in R functions we can see that there are 652446    travel records by operators in the  acme-travel-vacation file.
The travel csv file contains 16 features:
    The Destination of the trip (DESTINATION)
    The property id destination (PROPERTY_ID)
    The party size of the customers (PARTY_SIZE)
    The flight destination 3 letter code (MAIN_FLIGHT_DESTINATION)
    The start date of the trip (START_DATE)
    The length of the trip (LENGHT_OF_STAY)
    The date that the trip was booked(BKG_DATE)
    The revenue made from the trip (REVENUE)
    The total cost of the trip(TOTAL_COST)
    The profit margin for the agent(MARGIN)
    The star rating for the accomodation(ACCOMMODATION_STAR_RATING)
    The chain that the hotel is affiliated with(HOTEL_CHAIN_AFFILIATION)
    The time to event(TTE)
    The price per night for the trip(Price_PerNight)
    The booking date as a weekday(Wday_BookingDate)
    The start date as a weekday(Wday_StartDate)
    Overall, there are 652446 rows in our data, there are also 16 variables in our datasett

## The summary statistcs of the raw dataset is shown above.
```{r, message = FALSE}
head(raw, 12)
```

```{r, message = FALSE, fig.width=8, fig.align='left'}
a=table(raw$DESTINATION)
barplot(a,main="Booking Count per Destination",
        ylab="Count",
        xlab="Destination",
        col=rainbow(5),
        )
```
## Feature Engineering

## 3. Data Preparation

#### a) Data Modification
  We modified the data quite heavily. Firstly we removed a bunch of columns that were completely NULL and then focused on identifying the data columns that would be useful for our business case. As we were doing our business case based on hotel booking we did not not need most of the data concerning the flight information or flight codes.

#### Column Removals
Based on the business problem of trying to find optimal booking window for the agent we had removed most of teh columns visually as they were not providing sufficent analysis to the model we were trying to create. This can be seen on the previous sections where removed the features.

```{r, message = FALSE}
# Remove Lines for the ID and Null X final Columns
# travel_data_full <- travel_data_full[2:32]
```

#### Scale Data Set 
We did not sclae the dataset as our model did not require scaling of data. We decided to remove the outliers and that seemed sufficent enough to get a good enough survivi=al analysis model. Hence Scaling was not included in this report.
```{r, message = FALSE}

```

#### Split Data into Train and Test Sets

Since we are using a semi supervised learning model, we are not able to use any kind of test and train split on the original datset and all the data shall be used for modelling purposes.

```{r, message = FALSE}
# prop.table(table(travel_data_train$diagnosis))*100
# prop.table(table(travel_data_test$diagnosis))*100
```

For feature enginnering we would be removing null features and
## Removing Null Values
We did a summary on the main datset and traced all the features which had null values and removed it from the original dataset.
```{r, message = FALSE}
summary(raw)
```
 This section shall remove the null columns that are not useful for the analysis.
```{r, message = FALSE}
dataset <- raw

# Remove the following columns
# Remove the following columns
dataset$PACKAGE_ID <- NULL
dataset$ACCOM_SIZE_PAID <- NULL
dataset$ACCOM_LEVEL_PAID <- NULL
dataset$ACCOM_TYPE_RECEIVED <- NULL
dataset$ACCOM_SIZE_RECEIVED <- NULL
dataset$ACCOM_LEVEL_RECEIVED <- NULL
dataset$STATUS <- NULL
dataset$RATE_HEADER_ID <- NULL
dataset$RATE_CODE <- NULL
dataset$LIST_PRICE <- NULL
dataset$MARKET <- NULL
dataset$RATE_GRP <- NULL
dataset$CXL_DATE <- NULL
dataset$SEND_DATE <- NULL
dataset$BKG_ID <- NULL
dataset$AGENCY <- NULL
dataset$PACKAGE_TYPE_INDICATOR <- NULL
dataset$ACCOM_TYPE_PAID <- NULL
dataset$TRUE_ORIGIN <- NULL
dataset$MAIN_FLIGHT_ORIGIN <- NULL
#dataset$MAIN_FLIGHT_DESTINATION <- NULL
#dataset$MAIN_FLIGHT_DESTINATION <- NULL
dataset$INBOUND_FLIGHT_NUMBER <- NULL
dataset$INBOUND_FEEDER_FLIGHT_NUMBER <- NULL
dataset$INBOUND_TRAVEL_CLASS <- NULL
dataset$OUTBOUND_FLIGHT_NUMBER <- NULL
dataset$OUTBOUND_FEEDER_FLIGHT_NUMBER <- NULL
dataset$OUTBOUND_TRAVEL_CLASS <- NULL
dataset$BKG_TYPE <- NULL
dataset$SURCHARGES_AND_TAXES <- NULL
dataset$ANCILLARY_REVENUE <- NULL
#dataset$TOTAL_COST <- NULL
dataset$SOURCE <- NULL
```
## Data Transformation
We are performing transformation on all the date column by converting all 8 digit integer to  y/m/d format. This is important step as it allows our model to deciopher dates properly in correct format.
```{r, message = FALSE}
# Make certain adjustments to the data in the following columns
dataset$START_DATE <- as.Date(as.character(dataset$START_DATE), format="%Y%m%d")
# BKG_DATE
dataset$BKG_DATE <- as.Date(as.character(dataset$BKG_DATE), format="%Y%m%d")
# ACCOMMODATION_STAR_RATING
dataset$ACCOMMODATION_STAR_RATING <- gsub("\\*", "", dataset$ACCOMMODATION_STAR_RATING)
dataset$ACCOMMODATION_STAR_RATING <- as.factor(dataset$ACCOMMODATION_STAR_RATING)

# TTE - Time between vacation start and booking date
dataset$TTE <- dataset$START_DATE - dataset$BKG_DATE
dataset$TTE <- as.numeric(dataset$TTE)
# Price 
dataset$Price_PerNight <- dataset$REVENUE / dataset$PARTY_SIZE / dataset$LENGTH_OF_STAY
summary(dataset$Price_PerNight)
# Day of week of booking
dataset$Wday_BookingDate <- weekdays(as.Date(dataset$BKG_DATE))
dataset$Wday_BookingDate <- as.factor(dataset$Wday_BookingDate)
# Day of week of start of stay
dataset$Wday_StartDate <- weekdays(as.Date(dataset$START_DATE))
dataset$Wday_StartDate <- as.factor(dataset$Wday_StartDate)
```
Looking at the first few values of each column along with last few records
```{r, message = FALSE}
head(dataset,25)
names(dataset)
str(dataset)
tail(dataset)
```
## Data Split
The data is split into yearly format to visualize the data year on year. This allows us to see what is variance in the data for each year and guides us to create a better Survival analysis model.
```{r, message = FALSE}
#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
All_2019 <- dataset[dataset$START_DATE < as.Date("2020-01-01") & 
                          dataset$START_DATE > as.Date("2018-12-31") #&
                          # dataset$DESTINATION=="CANCUN" &
                          # dataset$LENGTH_OF_STAY >= 1 &
                          # dataset$LENGTH_OF_STAY <= 14 &
                          # dataset$MARGIN > 0
                          ,]

All_2018 <- dataset[dataset$START_DATE < as.Date("2019-01-01") & 
                         dataset$START_DATE > as.Date("2017-12-31") #&
                         # #dataset$DESTINATION=="CANCUN" &
                         # dataset$LENGTH_OF_STAY >= 1 &
                         # dataset$LENGTH_OF_STAY <= 14 &
                         # dataset$MARGIN > 0
                       ,]

All_2017 <- dataset[dataset$START_DATE < as.Date("2018-01-01") & 
                         dataset$START_DATE > as.Date("2016-12-31") #&
                         # dataset$DESTINATION=="CANCUN" &
                         # dataset$LENGTH_OF_STAY >= 1 &
                         # dataset$LENGTH_OF_STAY <= 14 &
                         # dataset$MARGIN > 0
                       ,]

All_2016 <- dataset[dataset$START_DATE < as.Date("2017-01-01") & 
                         dataset$START_DATE > as.Date("2015-12-31") #&
                         # dataset$DESTINATION=="CANCUN" &
                         # dataset$LENGTH_OF_STAY >= 1 &
                         # dataset$LENGTH_OF_STAY <= 14 &
                         # dataset$MARGIN > 0
                       ,]

All_2015 <- dataset[dataset$START_DATE < as.Date("2016-01-01") & 
                         dataset$START_DATE > as.Date("2014-12-31") #&
                         # dataset$DESTINATION=="CANCUN" &
                         # dataset$LENGTH_OF_STAY >= 1 &
                         # dataset$LENGTH_OF_STAY <= 14 &
                         # dataset$MARGIN > 0
                       ,]
```
We have created our model above and ran it on all the training data by saving into seperate excel format.

```{r, message = FALSE}
dataset_flt <- dataset
write.csv(dataset_flt,'dataset_flt.csv')
write.csv(All_2019,'Vacation_2019.csv')
write.csv(All_2018,'Vacation_2018.csv')
write.csv(All_2017,'Vacation_2017.csv')
write.csv(All_2016,'Vacation_2016.csv')
write.csv(All_2015,'Vacation_2015.csv')
```
Graph Histogram of Popular Destinations per Booking
```{r, message = FALSE}
tmp <- dataset %>%
  group_by(DESTINATION) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
tmp <- head(tmp, n=10)
tmp
tmp %>%
  ggplot(aes(x=reorder(DESTINATION,n), y=n))+
  geom_bar(stat="identity",fill="indian red")+
  coord_flip()
```
## Graph Histogram of Popular Start Date of Bookings
```{r, message = FALSE}
tmp <- All_2019 %>%
  group_by(START_DATE) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
tmp <- head(tmp, n=10)
tmp
tmp %>%
  ggplot(aes(x=reorder(START_DATE,n), y=n))+
  geom_bar(stat="identity",fill="indian red")+
  coord_flip()
```
## Graph Histogram of Popular Destinations per Booking
```{r, message = FALSE}

tmp <- All_2019 %>%
  group_by(HOTEL_CJAIN_AFFILIATION) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
tmp <- head(tmp, n=10)
tmp
tmp %>%
  ggplot(aes(x=reorder(HOTEL_CJAIN_AFFILIATION,n), y=n))+
  geom_bar(stat="identity",fill="indian red")+
  coord_flip()

```
## 4. Data Modeling

- Ethical Framework Questions: 
  - Does your use case require a more interpretable algorithm? 
    No, the algorithm that we are using is interpretable for the audience that we are targeting. 
    
  - Should you be optimizing for a different outcome than accuracy to make your outcomes fairer?
    No, as this is something that affects potential profit for companies accuracy is very important.
    
  - Is it possible that a malicious actor has compromised training data and created misleading results? 
    We thoroughly went through the data and made sure that any outliers or suspicious entries were dealt with accordingly.

## 4. A) Data Modeling Survival Analysis

#### Build a Time to Event Model based on all values to start.
#### This gives us a model that is reponsible to giving us optimal time to book a hotel

### Kaplan-Meier Model

According to IBM The Kaplan-Meier procedure is a method of estimating time-to-event models in the presence of censored cases. The Kaplan-Meier model is based on estimating conditional probabilities at each time point when an event occurs and taking the product limit of those probabilities to estimate the survival rate at each point in time.

We perfomed Kaplan-Meier Survival Analysis to identofy the period where the agent shall
```{r, message = FALSE}
km_2017 <- survfit(Surv(All_2017$TTE)~1)
#summary(km_2017)
plot(km_2017,conf.int=FALSE, mark.time=TRUE,main="Kaplan-Meier estimate of the Pre Booking 2017", xlab="Pre Booking in Days", ylab="Estimated probability")
```


```{r, message = FALSE}
km_2018 <- survfit(Surv(All_2018$TTE)~1)
#summary(km_2018)
plot(km_2018,conf.int=FALSE, mark.time=TRUE,main="Kaplan-Meier estimate of the Pre Booking 2018", xlab="Pre Booking in Days", ylab="Estimated probability")
```


```{r, message = FALSE}
km_2019 <- survfit(Surv(All_2019$TTE)~1)
summary(km_2019)
plot(km_2019,conf.int=FALSE, mark.time=TRUE,main="Kaplan-Meier estimate of the Pre Booking 2019", xlab="Pre Booking in Days", ylab="Estimated probability")
```


## Box Cox Model

A Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn’t normal, applying a Box-Cox means that you are able to run a broader number of tests.

```{r, message = FALSE}
plot_missing(raw)
```
This graph shows summary statistics for all the features we have in our dataset with regards to frequency of occurances.
```{r, message = FALSE}
plot_histogram(raw)
```
When Analyzing the dataset we see that few outliers show up 
```{r, message = FALSE}
plot_qq(raw, sampled_rows = 1000L)
```
Correlation graph to see any correlation between the data
```{r, message = FALSE}
plot_correlation(na.omit(raw), maxcat = 10L)
```
The boxplot gives ability to deciphers the outliers from the dataset
```{r, message = FALSE}
plot_boxplot(raw, by = "START_DATE")
```


```{r, message = FALSE}
##########################################################################
# Get the list of all the DESTINATION from the Raw data set
Name_of_Dest <- levels(dataset$DESTINATION)
Name_of_Dest
# Get the count of the DESTINATION from the Raw data set
Num_of_Dest <- length(Name_of_Dest)
Num_of_Dest
# Values for the TTE Dataframe
df_TTE <- NULL      # DataFrame
TTE_v <- NULL       # TTE Value of interest
dest_dest_v <- NULL # Destination to match in the airports.csv for map data
dest_airport_v <- NULL# Main Destination airport to match with file as Destination names may not match

for(i in 1:Num_of_Dest) {
  # Get Destination Label
  dest_dest <- Name_of_Dest[i]
  dest_dest
  Destination_num <- dataset[dataset$DESTINATION==dest_dest,]
  dest_airport <- as.character(Destination_num$MAIN_FLIGHT_DESTINATION[1])
  dest_airport
  # Count the number of rows in the data frame
  cnt <- nrow(Destination_num)
  # Perform the Kaplan-Meier estimate of the Pre Booking 
  km_curv <- survfit(Surv(Destination_num$TTE)~1)
  res <- summary(km_curv)
  # Create a data frame with only the Destination and TTE
  cols <- lapply(c(2:6), function(x) Destination_num[x])
  # Extract the columns you want
  cols <- lapply(c(2,6) , function(x) res[x])
  # Combine the columns into a data frame
  tbl <- do.call(data.frame, cols)
  # Extract values at % Required TODO: We could use this as a variable for more dynamic app?
  tbl_5pct <- tbl[tbl$surv < 0.05,]
  # Get the TTE as the first value in the subset
  TTE <- tbl_5pct$time[1]
  TTE_v <- c(TTE_v, TTE)
  dest_dest_v <- c(dest_dest_v, dest_dest)
  dest_airport_v <- c(dest_airport_v, dest_airport)
}
# dest_dest_v <- as.factor(dest_dest_v)
dest_airport_v <- as.character(dest_airport_v)
dest_dest_v <- as.character(dest_dest_v)
TTE_v <- as.character(TTE_v)
df_TTE <- data.frame("Destination" = dest_dest_v,"IATA" = dest_airport_v, "TTE" = TTE_v)
write.csv(df_TTE,'TTE_5pct.csv')
```

```{r, message = FALSE}
# Merge airport data for the dataset
# Airport File Raw
raw_airport = read.csv("./input/cities_IATA_long_lat.csv", header=TRUE)
df_IATA <- raw_airport
# Airport Code and ETA
TTE_aiport = read.csv("./input/IATA_TTE_5pct.csv", header=TRUE)
df_IATA_TTE <- TTE_aiport
# Merge two datasets
mydata <- merge(df_IATA,df_IATA_TTE,by="IATA")
mydata$X <- NULL
mydata$Destination <- NULL
write.csv(mydata,'IATA_TTE_5pct.csv')
```
## preprocess the data for modeling
```{r}
ggplot(data = All_2019) +
  geom_bar(mapping = aes(x=START_DATE))

ggplot(data = All_2018) +
  geom_bar(mapping = aes(x=START_DATE))

ggplot(data = All_2017) +
  geom_bar(mapping = aes(x=START_DATE))

ggplot(data = All_2019) +
  geom_bar(mapping = aes(x=TTE))

ggplot(data = All_2018) +
  geom_bar(mapping = aes(x=TTE))

ggplot(data = All_2017) +
  geom_bar(mapping = aes(x=TTE))

ggplot(data = All_2019) +
  geom_point(mapping = aes(x=TTE, y=ACCOMMODATION_STAR_RATING)) 

f <- ggplot(All_2019, aes(TTE, ACCOMMODATION_STAR_RATING))
f + geom_jitter() 

ggplot(data = All_2018) +
  geom_point(mapping = aes(x=TTE, y=ACCOMMODATION_STAR_RATING))

ggplot(data = All_2017, mapping = aes(x = ACCOMMODATION_STAR_RATING, y = TTE)) + 
  geom_boxplot(mapping = aes(group = cut_width(ACCOMMODATION_STAR_RATING, 0.1)))

ggplot(data = All_2018, mapping = aes(x = ACCOMMODATION_STAR_RATING, y = Price_PerNight)) + 
  geom_boxplot(mapping = aes(group = cut_width(ACCOMMODATION_STAR_RATING, 0.1)))

ggplot(data = All_2019, mapping = aes(x = ACCOMMODATION_STAR_RATING, y = TTE)) + 
  geom_boxplot(mapping = aes(group = cut_width(ACCOMMODATION_STAR_RATING, 0.1)))


All_2018$ACCOMMODATION_STAR_RATING <- as.factor(All_2018$ACCOMMODATION_STAR_RATING) 
ggplot(data = All_2018) +
  geom_point(mapping = aes(x= TTE, y = Price_PerNight))
str(All_2018)

str(All_2019)
ggplot(data = All_2019) +
  geom_point(mapping = aes(x= TTE, y = Price_PerNight, colour = ACCOMMODATION_STAR_RATING))


d <- ggplot(data=All_2019, aes(x=START_DATE, y=TTE,
                                  colour=ACCOMMODATION_STAR_RATING)) 
d + geom_point() + 
  geom_smooth(fill=NA, size=1.2)
```

## Final Data Preparation for Box Cox Model
```{r, message = FALSE, fig.width=8}
df <- raw[c("DESTINATION", "PROPERTY_ID", "START_DATE", "PARTY_SIZE", "LENGTH_OF_STAY", "BKG_DATE", "MARGIN")]
#df["PRE_BKG_DAYS"] <- df["START_DATE"] - df["BKG_DATE"]
# df["START_YEAR"] <- substr(df$START_DATE, 1, 4) # - as.Date(df$BKG_DATE,"%Y%m%d")
# df["START_MONTH"] <- substr(df$START_DATE, 5, 6)
# df["START_DAY_OF_MTH"] <- substr(df$START_DATE, 7, 8)
# df["START_MTH_AND_DAY"] <- substr(df$START_DATE, 5, 8)
str(df)
df_group <- aggregate(df[,4:7], df[,1:3], FUN = sum )
# df_group <- aggregate(df[-c("PROPERTY_ID","START_DATE")], df[c("PROPERTY_ID","START_DATE")], FUN = sum )
#df_group["MAX_OCCP"] <- df_group["PARTY_SIZE"]
#aggregate(df$MAX_OCCP, by = list(df$PROPERTY_ID), max)
#tapply(df_group$Value, df$PROPERTY_ID, max)
# df_group %>%
#   group_by(PROPERTY_ID) %>%
#   summarise(MAX_OCCP = max(PARTY_SIZE))
df_group["START_YEAR"] <- substr(df_group$START_DATE, 1, 4) # - as.Date(df$BKG_DATE,"%Y%m%d")
df_group["START_MONTH"] <- substr(df_group$START_DATE, 5, 6)
df_group["START_DAY_OF_MTH"] <- substr(df_group$START_DATE, 7, 8)
df_group["START_MTH_AND_DAY"] <- substr(df_group$START_DATE, 5, 8)
str(df_group)
kable(head(df_group,20))
kable(tail(df_group,20))
summary(df_group)
```

```{r, message = FALSE, fig.width=10, fig.height=8}
ggplot(aes(x=START_MTH_AND_DAY,y=PARTY_SIZE),data=df_group)+
  geom_point()
```

```{r, message = FALSE, fig.width=10, fig.height=8}
df_group %>%
  na.omit() %>%
ggplot(aes(x = START_MONTH, y = PARTY_SIZE)) +
      geom_point(color = "darkorchid4") +
      facet_wrap( ~ DESTINATION ) +
      labs(title = "Occupancy by Month for Destinations",
           subtitle = "Use facets to plot by a variable - DESTINATION in this case",
           y = "Daily Occupancy",
           x = "Date") + theme_bw(base_size = 15) #+
     # adjust the x axis breaks
     #scale_x_date(date_breaks = "5 years", date_labels = "%m-%Y")
```

```{r, message = FALSE}
# Make certain adjustments to the data in the following columns
df_group$START_DATE <- as.Date(as.character(df_group$START_DATE), format="%Y%m%d")
# BKG_DATE
df_group$BKG_DATE <- as.Date(as.character(df_group$BKG_DATE), format="%Y%m%d")

# TTE - Time between vacation start and booking date
df_group$TTE <- df_group$START_DATE - df_group$BKG_DATE
df_group$TTE <- as.numeric(df_group$TTE)

# Day of week of booking
df_group$Wday_BookingDate <- weekdays(as.Date(df_group$BKG_DATE))
df_group$Wday_BookingDate <- as.factor(df_group$Wday_BookingDate)
# Day of week of start of stay
df_group$Wday_StartDate <- weekdays(as.Date(df_group$START_DATE))
df_group$Wday_StartDate <- as.factor(df_group$Wday_StartDate)
```

```{r, message = FALSE}
names(df_group)
box_cox<- coxph(Surv(TTE) ~ 1 , data = df_group)
summary(box_cox)
curve_cox <- survfit(box_cox)
N <- length(unique(lung$PROPERTY_ID))
group <- lung$PROPERTY_ID
plot(curve_cox,xlab="Time",ylab="Survival Probability", mark.time = F, col=1:N)
# legend(
#   "topright",
#   legend=unique(group),
#   col=1:N,
#   horiz=FALSE,
#   bty='n', lty=2:3)

#plot(curve_cox)

```


## 6. Final Model Analysis and Selection

#### Cost Analysis

We cost for getting incorrect model would be general mistrust on the application by the agents and loss of revenue for the hotel and the travel agents in genrat. There is no other cost that can be assicated with this model.As this could potentially affect the business profit and bottom line, we need to try and keep error to a minimum.
```{r, message = FALSE}

```

#### Model Comparison

We have used the following two models to do prediction for the instacart shopping use case.The following Machine Learning Algorithms were used in this analysis:
  Kaplan-Meier - According to IBM The Kaplan-Meier procedure is a method of estimating time-to-event models in the presence of censored cases. The Kaplan-Meier model is based on estimating conditional probabilities at each time point when an event occurs and taking the product limit of those probabilities to estimate the survival rate at each point in time.
  Box Cox - A Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn’t normal, applying a Box-Cox means that you are able to run a broader number of tests.
  
KM estimates the survival curve for one group only. KM doesnt provide any kind of comparison between groups. As we are only looking at prebooking dates for our model KM is a better fit for us compared to box cox that works with multiple groups.

```{r, message = FALSE}

```

#### Selected Model: 
We selected Kaplier-Maier model as it gives us much comfortable results that we can take it to the app and explain it to the end client.
## 7. Deployment

#### Shiny App Url: 
https://csml1000-group8.shinyapps.io/CSML1000-Group8-CourseProject/
#### Summary Explanation

- Limitations of our analysis: 
  - Due to processing and resource limitations we used a full datset but removed lot of features and split it into years.
  - The analysis is based on data provided by Vaccation Company and may inherit any biases that exists in their customer base relative to the general population.
  
- Further steps: 
  - The analysis can be expanded to include all of the original data as well as any other similar sources that may be available. We also wanted to tackle the Ethical framework for the model by adding the percentages rules along with our time to event dates. The percentage would allow us to protect actors from creatiung huge nmistakes for themselves which would either be underiable, illegal or dangerous for the actors.

- Ethical Framework Questions: 
  - Can a malicious actor infer information about individuals from your system? No. There is no PII present.
  - Are you able to identify anomalous activity on your system that might indicate a security breach? This would need to be considered for each specific deployment.
  - Do you have a plan to monitor for poor performance on individuals or subgroups? N/A since No demographic data is present.
  - Do you have a plan to log and store historical predictions if a consumer requests access in the future? N/A since No demographic data is present.
  - Have you documented model retraining cycles and can you confirm that a subject’s data has been removed from models? N/A since No demographic data is present.


## References

Yihui Xie, J. J. Allaire, Garrett Grolemund, 2019, R Markdown: The Definitive Guide
https://bookdown.org/yihui/rmarkdown/markdown-syntax.html

Jonathan McPherson, 2016, R Notebooks
https://blog.rstudio.com/2016/10/05/r-notebooks

Adam Kardash, Patricia Kosseim, 2018, Responsible AI in Consumer Enterprise, integrate.ai

J Marcus W. Beck, 2018, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6262849/
