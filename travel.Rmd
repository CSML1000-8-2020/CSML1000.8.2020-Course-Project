---
title: "CSML1000 Winter 2020, Group 8, Course Project"
author: "Steven Wang, Tarun Bagga, Paul Doucet, Jerry Khidaroo, Nicola Stevanovic"
date: "3/19/2020"
# output: 
#   html_document:
#   toc: TRUE
#   toc_depth: 2
# runtime: shiny
output:
  pdf_document:
    toc: TRUE
    toc_depth: 3
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r, message = FALSE}
# Load packages
library(knitr)
library(arules)
library(arulesViz)
library(dplyr)
library(data.table)
library(shinydashboard)
library(shiny)
library(ggplot2)
library(lubridate)
#library(Hmisc)
#library(funModeling) 
library(tidyverse)
library(survival)
library(broom)

library(survival)
library(ranger)
library(ggplot2)
library(dplyr)
library(ggfortify)
```

## 1. Business Understanding

- Business Problem:
  The business problem we are trying to solve fir the final project is to facioptimizing travel agent bookings so that they can accurately gauge when  the surge will occur for the consummers to fly towards sun destinations and make a profit while doing so. To do so we will build a model that will  take historical travel data and use that to let travel agents know how far in advance they should prebook hotel rooms in popular travel locations.

- Project Plan: 
  - Load and get an understanding of the dataset, its target variable and its features.
  - Make any modifications to the dataset needed to enable learning algorithms to be run on the data.
  - Identify the features of the dataset that are important in predicting the target variable (dianosis in this case).
  - Build and evaluate a few models from the dataset by appling various machine learning algoritms as appropiate and    testing them.
  - Identify the best model to use for the project.
  - Build a shiny app that deploys the selected model with a user interface for end users to imput measurement values from  a study and obtain a pridiction result.
  - Identify any ethical considerations that should be addressed at each stage of the process.

- Business Success Criteria: 
  Success criteria for this business case would be that travel agents can use the application to streamline their booking process and also use the data provided to increase profitability by booking aggressively at hotels before surge takes place.

- Ethical Framework Questions: 
  - How could your system negatively impact individuals?
    This is very important question to ask as we have to look at how our recommendations are not creating biases or undesired negative results for the customers.As this system is based on historical data, some years might have had outside factors that skew the data and while an effort has been made to remove outliers there is still a chance that a booking date will be shown that is not the best predicted date for a possible destination.
  
  - Who is most vulnerable and why? 
    The most vulnerable people are the travel agents who would use the application, a common use case shall be that the booking agents try to prebook rooms expecting a lot of sales to take place in the coming months, in case the model has not predicted an extreme event to take place such as pandemic or a weather related event then they might not reach the expected amount of sales and will either lose money or not profit as much as possible.
  
  - How much error in predictions can your business accept for this use case? 
  As this could potentially affect the business profit and bottom line, we need to try and keep error to a minimum.
  
  - Will you need to explain which input factors had the greatest influence on outputs? 
  Yes, the important input factors will need to be explained as the model is time to event and we need to imagine where the data is coming from and data.
  
  - Do you need PII or can you provide group-level data? The analysis requires customers travel booking and booking costs data however any PII can be anonymised.

## 2. Data Understanding

- Ethical Framework Questions: 
  - Have you de-identified your data and taken measures to reduce the probability of reidentification? 
    Yes the data is de-identified
      
  - Will socially sensitive features like gender or ethnic background influence outputs? 
  No, socially sensitive features will not influence outputs
    
  - Are seemingly harmless features like location hiding proxies for socially sensitive features? 
  No, there are no socially sensitive features

#### 2.1 Get Data File

- For this final project we wanted to look at a real use case which presented real issues with data with skewness, data imbalance and ambiguity we came across the vaccations Dataset used is obtained from: Air Canada Hackathon Data

#### 2.2 Load and check data

```{r, message = FALSE}
# Load data
unzip("./input/acme-travel-vacation.zip", exdir = "input") # unzip file
raw = read.table("./input/acme-travel-vacation.csv", sep="\t", header=TRUE)

# Load in airport dataset
raw_airport = read.csv("./input/cities_IATA_long_lat.csv", header=TRUE)
dataset_airport <- raw_airport

# check data
str(raw)
```
#### 2.3 Initial Data Collection Report:

  - There is two files provided as part of the given dataset for this semi supervised learning model:
    1. acme-travel-vacation.csv
    2. cities_IATA_long_lat.csv
      Each of the datset contains different type of times series data for historical bookeing oif sun destination hotels by various travel operators.
      
## Let us explore and deep dive within the various datasets provided.
    
    For this section there are 2 CSV files provided, namely  acme-travel-vacation and cities_IATA_long_lat. The data inside the files specifies which . That is, order_products_prior contains previous order products for all customers and order_products_train contains the latest order products for some customers only.
    
Let us look the counts of records inside the files. Using the built-in R functions we can see that there are 652446    travel records by operators in the  order_products_prior file.
Both the CSV files contain 4 features:
    The ID of the order (order_id)
    The ID of the product (product_id)
    The ordering of that product in the order (add_to_cart_order)
    Whether that product was reordered (reordered).
    Overall, there are 3,346,083 unique orders for 49,685 unique products.There are 42 variables in this dataset
```{r, message = FALSE}
summary(raw)
```
## The summary statistcs of the raw dataset is shown above.
```{r, message = FALSE}
head(raw, 12)
```

```{r, message = FALSE, fig.width=8, fig.align='left'}
a=table(raw$DESTINATION)
barplot(a,main="Booking Count per Destination",
        ylab="Count",
        xlab="Destination",
        col=rainbow(5),
        #legend=rownames(a)
        )
```

```{r, message = FALSE}
dataset <- raw

# Remove the following columns
# Remove the following columns
dataset$PACKAGE_ID <- NULL
dataset$ACCOM_SIZE_PAID <- NULL
dataset$ACCOM_LEVEL_PAID <- NULL
dataset$ACCOM_TYPE_RECEIVED <- NULL
dataset$ACCOM_SIZE_RECEIVED <- NULL
dataset$ACCOM_LEVEL_RECEIVED <- NULL
dataset$STATUS <- NULL
dataset$RATE_HEADER_ID <- NULL
dataset$RATE_CODE <- NULL
dataset$LIST_PRICE <- NULL
dataset$MARKET <- NULL
dataset$RATE_GRP <- NULL
dataset$CXL_DATE <- NULL
dataset$SEND_DATE <- NULL
dataset$BKG_ID <- NULL
dataset$AGENCY <- NULL
dataset$PACKAGE_TYPE_INDICATOR <- NULL
dataset$ACCOM_TYPE_PAID <- NULL
dataset$TRUE_ORIGIN <- NULL
dataset$MAIN_FLIGHT_ORIGIN <- NULL
#dataset$MAIN_FLIGHT_DESTINATION <- NULL
#dataset$MAIN_FLIGHT_DESTINATION <- NULL
dataset$INBOUND_FLIGHT_NUMBER <- NULL
dataset$INBOUND_FEEDER_FLIGHT_NUMBER <- NULL
dataset$INBOUND_TRAVEL_CLASS <- NULL
dataset$OUTBOUND_FLIGHT_NUMBER <- NULL
dataset$OUTBOUND_FEEDER_FLIGHT_NUMBER <- NULL
dataset$OUTBOUND_TRAVEL_CLASS <- NULL
dataset$BKG_TYPE <- NULL
dataset$SURCHARGES_AND_TAXES <- NULL
dataset$ANCILLARY_REVENUE <- NULL
#dataset$TOTAL_COST <- NULL
dataset$SOURCE <- NULL
```

```{r, message = FALSE}
# Make certain adjustments to the data in the following columns
dataset$START_DATE <- as.Date(as.character(dataset$START_DATE), format="%Y%m%d")
# BKG_DATE
dataset$BKG_DATE <- as.Date(as.character(dataset$BKG_DATE), format="%Y%m%d")
# ACCOMMODATION_STAR_RATING
dataset$ACCOMMODATION_STAR_RATING <- gsub("\\*", "", dataset$ACCOMMODATION_STAR_RATING)
dataset$ACCOMMODATION_STAR_RATING <- as.factor(dataset$ACCOMMODATION_STAR_RATING)

# TTE - Time between vacation start and booking date
dataset$TTE <- dataset$START_DATE - dataset$BKG_DATE
dataset$TTE <- as.numeric(dataset$TTE)
# Price 
dataset$Price_PerNight <- dataset$REVENUE / dataset$PARTY_SIZE / dataset$LENGTH_OF_STAY
summary(dataset$Price_PerNight)
# Day of week of booking
dataset$Wday_BookingDate <- weekdays(as.Date(dataset$BKG_DATE))
dataset$Wday_BookingDate <- as.factor(dataset$Wday_BookingDate)
# Day of week of start of stay
dataset$Wday_StartDate <- weekdays(as.Date(dataset$START_DATE))
dataset$Wday_StartDate <- as.factor(dataset$Wday_StartDate)
```

```{r, message = FALSE}
head(dataset,25)
names(dataset)
str(dataset)
tail(dataset)
```

```{r, message = FALSE}
#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
All_2019 <- dataset[dataset$START_DATE < as.Date("2020-01-01") & 
                          dataset$START_DATE > as.Date("2018-12-31") #&
                          # dataset$DESTINATION=="CANCUN" &
                          # dataset$LENGTH_OF_STAY >= 1 &
                          # dataset$LENGTH_OF_STAY <= 14 &
                          # dataset$MARGIN > 0
                          ,]

All_2018 <- dataset[dataset$START_DATE < as.Date("2019-01-01") & 
                         dataset$START_DATE > as.Date("2017-12-31") #&
                         # #dataset$DESTINATION=="CANCUN" &
                         # dataset$LENGTH_OF_STAY >= 1 &
                         # dataset$LENGTH_OF_STAY <= 14 &
                         # dataset$MARGIN > 0
                       ,]

All_2017 <- dataset[dataset$START_DATE < as.Date("2018-01-01") & 
                         dataset$START_DATE > as.Date("2016-12-31") #&
                         # dataset$DESTINATION=="CANCUN" &
                         # dataset$LENGTH_OF_STAY >= 1 &
                         # dataset$LENGTH_OF_STAY <= 14 &
                         # dataset$MARGIN > 0
                       ,]

All_2016 <- dataset[dataset$START_DATE < as.Date("2017-01-01") & 
                         dataset$START_DATE > as.Date("2015-12-31") #&
                         # dataset$DESTINATION=="CANCUN" &
                         # dataset$LENGTH_OF_STAY >= 1 &
                         # dataset$LENGTH_OF_STAY <= 14 &
                         # dataset$MARGIN > 0
                       ,]

All_2015 <- dataset[dataset$START_DATE < as.Date("2016-01-01") & 
                         dataset$START_DATE > as.Date("2014-12-31") #&
                         # dataset$DESTINATION=="CANCUN" &
                         # dataset$LENGTH_OF_STAY >= 1 &
                         # dataset$LENGTH_OF_STAY <= 14 &
                         # dataset$MARGIN > 0
                       ,]


# Save the New Dataset
dataset_flt <- dataset
write.csv(dataset_flt,'dataset_flt.csv')
write.csv(All_2019,'Vacation_2019.csv')
write.csv(All_2018,'Vacation_2018.csv')
write.csv(All_2017,'Vacation_2017.csv')
write.csv(All_2016,'Vacation_2016.csv')
write.csv(All_2015,'Vacation_2015.csv')
```

```{r, message = FALSE}
# Graph Histogram of Popular Destinations per Booking
tmp <- dataset %>%
  group_by(DESTINATION) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
tmp <- head(tmp, n=10)
tmp
tmp %>%
  ggplot(aes(x=reorder(DESTINATION,n), y=n))+
  geom_bar(stat="identity",fill="indian red")+
  coord_flip()
```

```{r, message = FALSE}
# EDA - Complete Dataset
#glimpse1 <- glimpse(dataset)
#df_status1 <- df_status(dataset)
#freq1 <- freq(dataset)
#profiling_num1 <- profiling_num(dataset)
#plot1 <- plot_num(dataset)
#describe1 <- describe(dataset)
```


```{r, message = FALSE}
# Graph Histogram of Popular Start Date of Bookings
tmp <- All_2019 %>%
  group_by(START_DATE) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
tmp <- head(tmp, n=10)
tmp
tmp %>%
  ggplot(aes(x=reorder(START_DATE,n), y=n))+
  geom_bar(stat="identity",fill="indian red")+
  coord_flip()
```


```{r, message = FALSE}
# Graph Histogram of Popular Destinations per Booking
tmp <- All_2019 %>%
  group_by(HOTEL_CJAIN_AFFILIATION) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
tmp <- head(tmp, n=10)
tmp
tmp %>%
  ggplot(aes(x=reorder(HOTEL_CJAIN_AFFILIATION,n), y=n))+
  geom_bar(stat="identity",fill="indian red")+
  coord_flip()

```


```{r, message = FALSE}
km_2017 <- survfit(Surv(All_2017$TTE)~1)
summary(km_2017)
plot(km_2017,conf.int=FALSE, mark.time=TRUE,main="Kaplan-Meier estimate of the Pre Booking 2017", xlab="Pre Booking in Days", ylab="Estimated probability")
```


```{r, message = FALSE}
km_2018 <- survfit(Surv(All_2018$TTE)~1)
summary(km_2018)
plot(km_2018,conf.int=FALSE, mark.time=TRUE,main="Kaplan-Meier estimate of the Pre Booking 2018", xlab="Pre Booking in Days", ylab="Estimated probability")
```


```{r, message = FALSE}
km_2019 <- survfit(Surv(All_2019$TTE)~1)
summary(km_2019)
plot(km_2019,conf.int=FALSE, mark.time=TRUE,main="Kaplan-Meier estimate of the Pre Booking 2019", xlab="Pre Booking in Days", ylab="Estimated probability")
```



```{r, message = FALSE}
 km_curv <- survfit(Surv(Destination_num$TTE)~1)
 km_res <- summary(km_curv)
 
 str(Destination_num)
 
 cox <- coxph(Surv(Destination_num$TTE)~1 + Destination_num$DESTINATION + 
                Destination_num$PROPERTY_ID + 
                Destination_num$PARTY_SIZE +
                Destination_num$MAIN_FLIGHT_DESTINATION +
                Destination_num$START_DATE +
                Destination_num$LENGTH_OF_STAY #+
#                # Destination_num$BKG_DATE +
#                # Destination_num$REVENUE +
#                # Destination_num$TOTAL_COST +
#                # Destination_num$MARGIN +
#                # Destination_num$ACCOMMODATION_STAR_RATING +
#                # Destination_num$HOTEL_CJAIN_AFFILIATION + 
#                # Destination_num$Price_PerNight +
#                # Destination_num$Cost_PerNight
#                # 
              )
 cox_res <- summary(cox)

 cox_res

```


```{r, message = FALSE}
raw %>% filter(LENGTH_OF_STAY >= 1 & LENGTH_OF_STAY < 14)
# Profiling the data input
head(raw)
str(raw)
```

```{r, message = FALSE}
plot_str(raw)
```

```{r, message = FALSE}
plot_missing(raw)
```

```{r, message = FALSE}
plot_bar(raw)
```

```{r, message = FALSE}
plot_histogram(raw)
```

```{r, message = FALSE}
plot_qq(raw, sampled_rows = 1000L)
```

```{r, message = FALSE}
plot_correlation(na.omit(raw), maxcat = 10L)
```

```{r, message = FALSE}
plot_boxplot(raw, by = "START_DATE")
```

```{r, message = FALSE}
# M <- cor(travel_data_full)
# corrplot(M, method="circle", type="full")
```

## 3. Data Preparation

#### a) Data Modification
  We modified the data quite heavily. Firstly we removed a bunch of columns that were completely NULL and then focused on identifying the data columns that would be useful for our business case. As we were doing our business case based on hotel booking we did not not need most of the data concerning the flight information or flight codes.

#### Column Removals

```{r, message = FALSE}
# Remove Lines for the ID and Null X final Columns
# travel_data_full <- travel_data_full[2:32]
```

#### Scale Data Set 

```{r, message = FALSE}
# Encoding the target feature as factor
# travel_data_full$diagnosis = factor(travel_data_full$diagnosis,
#                                        levels = c('B', 'M'),
#                                        labels = c(0,1))

# Scaling the dataset for models that require it
# travel_data_scaled <- travel_data_full
# travel_data_scaled[,2:31] <- scale(travel_data_scaled[,2:31])
# str(travel_data_scaled)
# names(travel_data_full)
```

#### Split Data into Train and Test Sets

- Here we are splitting the datatsets in test and train datasets which alows for running the models.

```{r, message = FALSE}
# Split the data into a train set and a test set
# travel_data_train <- travel_data_full[1:426,]
# travel_data_test <- travel_data_full[427:569,]
```

#### The Dependent Variable

- The dependant variable in our analysis:

```{r, message = FALSE}
# prop.table(table(travel_data_train$diagnosis))*100
# prop.table(table(travel_data_test$diagnosis))*100
```

- 

#### b) Feature Engineering

#### View Correlation Matrix to explore highly correlated features

```{r, message = FALSE}
# M <- cor(diagnosis_data_train[,2:31])
# 24) perimeter_worst, 19) concave.points_worst, 22) radius_worst, 9) concave.points_mean, 8) concavity_mean
# M <- cor(diagnosis_data_full[24, 19, 22, 9, 8])
# corrplot(M, method="circle", type="full")
```

#### Check Feature variables distribution vs Target

```{r, message = FALSE}

# featurePlot(x = diagnosis_data_train[, 2:31], 
#             y = diagnosis_data_train$diagnosis, 
#             plot = "density", 
#             strip=strip.custom(par.strip.text=list(cex=.7)),
#             scales = list(x = list(relation="free"), 
#                           y = list(relation="free")), layout = c(6, 5), adjust = 1.5, pch = "|", auto.key=list(columns=2))
```

#### Try a recursive feature elimination check - Feature Selection Method 1

We tried multiple methods to decide which variables to use for feature importance. First one is recursive feature elimination technique as below.

```{r, message = FALSE}
# set.seed(100)
# options(warn=-1)
# 
# subsets <- c(1:5, 15, 20, 25, 31)
# 
# ctrl <- rfeControl(functions = rfFuncs,
#                    method = "repeatedcv",
#                    repeats = 5,
#                    verbose = FALSE)
# 
# lmProfile <- rfe(x=diagnosis_data_train[, 2:31], y=diagnosis_data_train$diagnosis,
#                  sizes = subsets,
#                  rfeControl = ctrl)
# 
# lmProfile
```

## 4. Data Modeling

- Ethical Framework Questions: 
  - Does your use case require a more interpretable algorithm? 
    No, the algorithm that we are using is interpretable for the audience that we are targeting. 
    
  - Should you be optimizing for a different outcome than accuracy to make your outcomes fairer?
    No, as this is something that affects potential profit for companies accuracy is very important.
    
  - Is it possible that a malicious actor has compromised training data and created misleading results? 
    We thoroughly went through the data and made sure that any outliers or suspicious entries were dealt with accordingly.

## 4. A) Data Modeling Time to Event

#### Build a Time to Event Model based on all values to start.
#### This gives us a model that is reponsible to giving us optimal time to book a hotel

```{r, message = FALSE}
##########################################################################
# Get the list of all the DESTINATION from the Raw data set
Name_of_Dest <- levels(dataset$DESTINATION)
Name_of_Dest
# Get the count of the DESTINATION from the Raw data set
Num_of_Dest <- length(Name_of_Dest)
Num_of_Dest
# Values for the TTE Dataframe
df_TTE <- NULL      # DataFrame
TTE_v <- NULL       # TTE Value of interest
dest_dest_v <- NULL # Destination to match in the airports.csv for map data
dest_airport_v <- NULL# Main Destination airport to match with file as Destination names may not match

for(i in 1:Num_of_Dest) {
  # Get Destination Label
  dest_dest <- Name_of_Dest[i]
  dest_dest
  Destination_num <- dataset[dataset$DESTINATION==dest_dest,]
  dest_airport <- as.character(Destination_num$MAIN_FLIGHT_DESTINATION[1])
  dest_airport
  # Count the number of rows in the data frame
  cnt <- nrow(Destination_num)
  # Perform the Kaplan-Meier estimate of the Pre Booking 
  km_curv <- survfit(Surv(Destination_num$TTE)~1)
  res <- summary(km_curv)
  # Create a data frame with only the Destination and TTE
  cols <- lapply(c(2:6), function(x) Destination_num[x])
  # Extract the columns you want
  cols <- lapply(c(2,6) , function(x) res[x])
  # Combine the columns into a data frame
  tbl <- do.call(data.frame, cols)
  # Extract values at % Required TODO: We could use this as a variable for more dynamic app?
  tbl_5pct <- tbl[tbl$surv < 0.05,]
  # Get the TTE as the first value in the subset
  TTE <- tbl_5pct$time[1]
  TTE_v <- c(TTE_v, TTE)
  dest_dest_v <- c(dest_dest_v, dest_dest)
  dest_airport_v <- c(dest_airport_v, dest_airport)
}
# dest_dest_v <- as.factor(dest_dest_v)
dest_airport_v <- as.character(dest_airport_v)
dest_dest_v <- as.character(dest_dest_v)
TTE_v <- as.character(TTE_v)
df_TTE <- data.frame("Destination" = dest_dest_v,"IATA" = dest_airport_v, "TTE" = TTE_v)
write.csv(df_TTE,'TTE_5pct.csv')
```


```{r, message = FALSE}
# Merge airport data for the dataset
# Airport File Raw
raw_airport = read.csv("./input/cities_IATA_long_lat.csv", header=TRUE)
df_IATA <- raw_airport
# Airport Code and ETA
TTE_aiport = read.csv("./input/TTE_5pct.csv", header=TRUE)
df_IATA_TTE <- TTE_aiport
# Merge two datasets
mydata <- merge(df_IATA,df_IATA_TTE,by="IATA")
mydata$X <- NULL
mydata$Destination <- NULL
write.csv(mydata,'IATA_TTE_5pct.csv')
```
## preprocess the data for modeling
```{r}
ggplot(data = All_2019) +
  geom_bar(mapping = aes(x=START_DATE))

ggplot(data = All_2018) +
  geom_bar(mapping = aes(x=START_DATE))

ggplot(data = All_2017) +
  geom_bar(mapping = aes(x=START_DATE))

ggplot(data = All_2019) +
  geom_bar(mapping = aes(x=TTE))

ggplot(data = All_2018) +
  geom_bar(mapping = aes(x=TTE))

ggplot(data = All_2017) +
  geom_bar(mapping = aes(x=TTE))

ggplot(data = All_2019) +
  geom_point(mapping = aes(x=TTE, y=ACCOMMODATION_STAR_RATING)) 

f <- ggplot(All_2019, aes(TTE, ACCOMMODATION_STAR_RATING))
f + geom_jitter() 

ggplot(data = All_2018) +
  geom_point(mapping = aes(x=TTE, y=ACCOMMODATION_STAR_RATING))

ggplot(data = All_2017, mapping = aes(x = ACCOMMODATION_STAR_RATING, y = TTE)) + 
  geom_boxplot(mapping = aes(group = cut_width(ACCOMMODATION_STAR_RATING, 0.1)))

ggplot(data = All_2018, mapping = aes(x = ACCOMMODATION_STAR_RATING, y = Price_PerNight)) + 
  geom_boxplot(mapping = aes(group = cut_width(ACCOMMODATION_STAR_RATING, 0.1)))

ggplot(data = All_2019, mapping = aes(x = ACCOMMODATION_STAR_RATING, y = TTE)) + 
  geom_boxplot(mapping = aes(group = cut_width(ACCOMMODATION_STAR_RATING, 0.1)))


All_2018$ACCOMMODATION_STAR_RATING <- as.factor(All_2018$ACCOMMODATION_STAR_RATING) 
ggplot(data = All_2018) +
  geom_point(mapping = aes(x= TTE, y = Price_PerNight))
str(All_2018)

str(All_2019)
ggplot(data = All_2019) +
  geom_point(mapping = aes(x= TTE, y = Cost_PerNight, colour = ACCOMMODATION_STAR_RATING))


d <- ggplot(data=All_2019, aes(x=START_DATE, y=TTE,
                                  colour=ACCOMMODATION_STAR_RATING)) 
d + geom_point() + 
  geom_smooth(fill=NA, size=1.2)
```


```{r, message = FALSE, fig.width=8}
df <- raw[c("DESTINATION", "PROPERTY_ID", "START_DATE", "PARTY_SIZE", "LENGTH_OF_STAY", "BKG_DATE", "MARGIN")]
#df["PRE_BKG_DAYS"] <- df["START_DATE"] - df["BKG_DATE"]
# df["START_YEAR"] <- substr(df$START_DATE, 1, 4) # - as.Date(df$BKG_DATE,"%Y%m%d")
# df["START_MONTH"] <- substr(df$START_DATE, 5, 6)
# df["START_DAY_OF_MTH"] <- substr(df$START_DATE, 7, 8)
# df["START_MTH_AND_DAY"] <- substr(df$START_DATE, 5, 8)
str(df)
df_group <- aggregate(df[,4:7], df[,1:3], FUN = sum )
# df_group <- aggregate(df[-c("PROPERTY_ID","START_DATE")], df[c("PROPERTY_ID","START_DATE")], FUN = sum )
#df_group["MAX_OCCP"] <- df_group["PARTY_SIZE"]
#aggregate(df$MAX_OCCP, by = list(df$PROPERTY_ID), max)
#tapply(df_group$Value, df$PROPERTY_ID, max)
# df_group %>%
#   group_by(PROPERTY_ID) %>%
#   summarise(MAX_OCCP = max(PARTY_SIZE))
df_group["START_YEAR"] <- substr(df_group$START_DATE, 1, 4) # - as.Date(df$BKG_DATE,"%Y%m%d")
df_group["START_MONTH"] <- substr(df_group$START_DATE, 5, 6)
df_group["START_DAY_OF_MTH"] <- substr(df_group$START_DATE, 7, 8)
df_group["START_MTH_AND_DAY"] <- substr(df_group$START_DATE, 5, 8)
str(df_group)
kable(head(df_group,20))
kable(tail(df_group,20))
summary(df_group)
```


## 6. Final Model Analysis and Selection

#### Cost Analysis


```{r, message = FALSE}

```

#### Model Comparison

<!-- - The following Machine Learning Algorithms were used in this analysis: -->
<!--                         Accuracy Sesitivity Specificity  -->
<!--   - Random Forest        0,9790    0.9722     1.0000 -->
<!--   - Logistic Regression  0.9578    0.9434     0.9664 -->
<!--   - Neural Net           0.9790    1.0000     0.9722 -->
<!--   - kernel SVM           0.9790    0.9722     1.0000 -->


<!-- Features of Random Forest:  -->
<!-- Random Forest is a classification and regression algorithm. Here, we train several decision trees. The original learning dataset is randomly divided into several subsets of equal size. A decision tree is trained for each subset. -->

<!-- Advantages: -->
<!--   - Robust to overfitting (thus solving one of the biggest disadvantages of decision trees) -->
<!--   - Parameterization remains quite simple and intuitive -->
<!--   - Performs very well when the number of features is big and for large quantity of learning data -->

<!-- Disadvantages: -->
<!--   - Models generated with Random Forest may take a lot of memory -->
<!--   - Learning may be slow (depending on the parameterization) -->
<!--   - Not possible to iteratively improve the generated models -->

<!-- Logistic Regression: -->
<!-- Logistic Regression Model is a generalized form of Linear Regression Model. It is a very good Discrimination Tool. Following are the advantages and disadvantage of Logistic Regression: -->

<!-- Advantages: -->
<!--   - Logistic Regression performs well when the dataset is linearly separable. -->
<!--   - Logistic regression is less prone to over-fitting but it can overfit in high dimensional datasets. You should consider Regularization (L1 and L2) techniques to avoid over-fitting in these scenarios. -->
<!--   - Logistic Regression not only gives a measure of how relevant a predictor (coefficient size) is, but also its direction of association (positive or negative). -->
<!--   - Logistic regression is easier to implement, interpret and very efficient to train. -->

<!-- Disadvantages: -->
<!--   - Main limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is rarely linearly separable. Most of the time data would be a jumbled mess. -->
<!--   - If the number of observations are lesser than the number of features, Logistic Regression should not be used, otherwise it may lead to overfit. -->
<!--   - Logistic Regression can only be used to predict discrete functions. Therefore, the dependent variable of Logistic Regression is restricted to the discrete number set. This restriction itself is problematic, as it is prohibitive to the prediction of continuous data. -->

<!-- Neural Network -->
<!-- Advantages: -->
<!--   - Complicated functions and non-linear problems can be solved easily by Neural network. -->
<!--   - Can use ensembling with other techniques to get good solution. -->
<!-- Disadvantage -->
<!--   - Much Slower for training and classification -->
<!--   - Hard to interpret, -->
<!--   - Data comes in streams -->
<!--   - Not usable with small datasets. -->

<!-- Kernel SVM -->
<!-- Features of SVMs: Support Vector machine is a classification algorithm used primarily with text classification problems. It uses  hyperplane to separate out different cluster of data. You can cut the universe in different classes using the hyperplane which can be molded in any direction. This can be done both linearly and non-linearly. The identified hyperplane can be thought as a decision boundary between the two clusters. This allows classification of vectors multi dimensionally. This can be used with text classification by encoding on text data. This results in every item in the dataset being represented as a vector with large value dimensions, everyone representing the frequency one of the words of the text. -->

<!-- Advantages: -->
<!--   - High accuracy with small data, -->
<!--   - Not susceptible to overfitting -->
<!--   - Works with linear and non-linear data. -->

<!-- Disadvantage: -->
<!--   - Memory-intensive operationally -->
<!--   - Hard to understand and implement certain. -->

```{r, message = FALSE}

```

#### Selected Model: 

## 7. Deployment

#### Shiny App Url: 

#### Summary Explanation

- Limitations of our analysis: 
  - 
  
- Further steps we could take: 
  - 
  
- Explanation of Model:
  - 

- Ethical Framework Questions: 
  - Can a malicious actor infer information about individuals from your system? 
  - Are you able to identify anomalous activity on your system that might indicate a security breach? 
  - Do you have a plan to monitor for poor performance on individuals or subgroups? 
  - Do you have a plan to log and store historical predictions if a consumer requests access in the future? 
  - Have you documented model retraining cycles and can you confirm that a subject’s data has been removed from models? 

## References

Yihui Xie, J. J. Allaire, Garrett Grolemund, 2019, R Markdown: The Definitive Guide
https://bookdown.org/yihui/rmarkdown/markdown-syntax.html

Jonathan McPherson, 2016, R Notebooks
https://blog.rstudio.com/2016/10/05/r-notebooks

Adam Kardash, Patricia Kosseim, 2018, Responsible AI in Consumer Enterprise, integrate.ai

J Marcus W. Beck, 2018, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6262849/
